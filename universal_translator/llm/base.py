from dataclasses import dataclass
from typing import List, Tuple


@dataclass
class Usage:
    total_tokens: int
    prompt_tokens: int
    completion_tokens: int

    def add(self, other: "Usage") -> "Usage":
        """
        Add two Usage objects together.

        Args:
            other (Usage): The Usage object to add to the current Usage object.

        Returns:
            Usage: A new Usage object with the total tokens, prompt tokens, and completion tokens summed up.
        """
        return Usage(
            total_tokens=self.total_tokens + other.total_tokens,
            prompt_tokens=self.prompt_tokens + other.prompt_tokens,
            completion_tokens=self.completion_tokens + other.completion_tokens,
        )

    def to_dict(self) -> dict:
        """
        Convert the Usage object to a dictionary.
        """
        return self.__dict__


class LLMProvider:
    def __init__(self) -> None:
        # Initialize the provider
        self.model: str = ""

    @staticmethod
    def calculate_chunk_size(token_count: int, token_limit: int) -> int:
        """
        Calculate the chunk size based on the token count and token limit.

        Args:
            token_count (int): The total number of tokens.
            token_limit (int): The maximum number of tokens allowed per chunk.

        Returns:
            int: The calculated chunk size.

        Description:
            This function calculates the chunk size based on the given token count and token limit.
            If the token count is less than or equal to the token limit, the function returns the token count as the chunk size.
            Otherwise, it calculates the number of chunks needed to accommodate all the tokens within the token limit.
            The chunk size is determined by dividing the token limit by the number of chunks.
            If there are remaining tokens after dividing the token count by the token limit,
            the chunk size is adjusted by adding the remaining tokens divided by the number of chunks.

        Example:
            >>> calculate_chunk_size(1000, 500)
            500
            >>> calculate_chunk_size(1530, 500)
            389
            >>> calculate_chunk_size(2242, 500)
            496
        """

        if token_count <= token_limit:
            return token_count

        num_chunks = (token_count + token_limit - 1) // token_limit
        chunk_size = token_count // num_chunks

        remaining_tokens = token_count % token_limit
        if remaining_tokens > 0:
            chunk_size += remaining_tokens // num_chunks

        return chunk_size

    def num_tokens_in_string(self, input_str: str) -> int:
        """
        Calculate the number of tokens in a given string using a specified encoding.

        Args:
            input_str: The input string to be tokenized.

        Returns:
            int: The number of tokens in the input string.
        """
        raise NotImplementedError("The num_tokens_in_string method must be implemented in a subclass.")

    def split_text(self, text: str, max_tokens: int = 1000) -> List[str]:
        """
        Split the input text into chunks of tokens.

        Args:
            text (str): The input text to be split.
            max_tokens (int, optional): The maximum number of tokens per chunk. Defaults to 1000.

        Returns:
            List[str]: A list of text chunks, each containing a maximum of `max_tokens` tokens.
        """
        raise NotImplementedError("The split_text method must be implemented in a subclass.")

    def get_completion(
        self,
        prompt: str,
        system_message: str = "You are a helpful assistant.",
    ) -> Tuple[str, Usage]:
        """
        Get the completion for a given prompt.

        Args:
            prompt (str): The prompt to generate a completion for.
            system_message (str, optional): The system message to include in the completion. Defaults to "You are a helpful assistant.".

        Returns:
            str: The completion generated by the language model
        """
        raise NotImplementedError("The get_completion method must be implemented in a subclass.")
